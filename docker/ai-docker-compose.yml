services:
  ollama-llm:
    image: ollama/ollama:latest
    container_name: ollama-llm
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ "gpu" ]
              count: all
    volumes:
      - ./ai/ollama:/data
    ports:
      - "11434:11434"
    networks:
      - ollama-network

  prepare-models:
    image: ollama/ollama:latest
    depends_on:
      - ollama-llm
    volumes:
      - ./ai/ollama:/data
    environment:
      - OLLAMA_HOST=http://ollama-llm:11434
    networks:
      - ollama-network
    entrypoint: >
      sh -c "
        echo 'Start pulling model...' &&
        ollama pull herenickname/t-tech_T-lite-it-1.0:q4_k_m &&
        echo 'pulling complete, run model' &&
        ollama run herenickname/t-tech_T-lite-it-1.0:q4_k_m
      "

networks:
  ollama-network:
    driver: bridge
